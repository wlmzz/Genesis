global:
  resolve_timeout: 5m
  smtp_smarthost: 'localhost:587'
  smtp_from: 'genesis-alerts@example.com'
  smtp_require_tls: false

route:
  group_by: ['alertname', 'severity', 'component']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default-receiver'

  routes:
    # Critical alerts - immediate notification
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 0s
      repeat_interval: 5m
      continue: true

    # Warning alerts - grouped notifications
    - match:
        severity: warning
      receiver: 'warning-alerts'
      group_interval: 5m
      repeat_interval: 1h

    # Info alerts - daily digest
    - match:
        severity: info
      receiver: 'info-alerts'
      group_interval: 1h
      repeat_interval: 24h

    # Component-specific routing
    - match:
        component: event-bus
      receiver: 'event-bus-alerts'
      continue: true

    - match:
        component: database
      receiver: 'database-alerts'
      continue: true

    - match:
        component: ml-ops
      receiver: 'mlops-alerts'
      continue: true

receivers:
  - name: 'default-receiver'
    webhook_configs:
      - url: 'http://api-gateway:8000/api/v1/alerts/webhook'
        send_resolved: true

  - name: 'critical-alerts'
    email_configs:
      - to: 'oncall@example.com'
        headers:
          Subject: '[CRITICAL] Genesis Alert: {{ .GroupLabels.alertname }}'
        html: |
          <h2>Critical Alert Triggered</h2>
          <p><strong>Alert:</strong> {{ .GroupLabels.alertname }}</p>
          <p><strong>Component:</strong> {{ .GroupLabels.component }}</p>
          <p><strong>Summary:</strong> {{ .CommonAnnotations.summary }}</p>
          <p><strong>Description:</strong> {{ .CommonAnnotations.description }}</p>
          <p><strong>Time:</strong> {{ .StartsAt }}</p>

    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#genesis-alerts-critical'
        title: 'CRITICAL: {{ .GroupLabels.alertname }}'
        text: |
          *Summary:* {{ .CommonAnnotations.summary }}
          *Description:* {{ .CommonAnnotations.description }}
          *Component:* {{ .GroupLabels.component }}
        send_resolved: true

    webhook_configs:
      - url: 'http://api-gateway:8000/api/v1/alerts/webhook/critical'
        send_resolved: true

  - name: 'warning-alerts'
    email_configs:
      - to: 'team@example.com'
        headers:
          Subject: '[WARNING] Genesis Alert: {{ .GroupLabels.alertname }}'

    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#genesis-alerts'
        title: 'Warning: {{ .GroupLabels.alertname }}'
        text: '{{ .CommonAnnotations.summary }}'

  - name: 'info-alerts'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#genesis-info'
        title: 'Info: {{ .GroupLabels.alertname }}'
        text: '{{ .CommonAnnotations.summary }}'

  - name: 'event-bus-alerts'
    email_configs:
      - to: 'backend-team@example.com'
        headers:
          Subject: '[Event Bus] {{ .GroupLabels.alertname }}'

    webhook_configs:
      - url: 'http://api-gateway:8000/api/v1/alerts/webhook/event-bus'

  - name: 'database-alerts'
    email_configs:
      - to: 'database-team@example.com'
        headers:
          Subject: '[Database] {{ .GroupLabels.alertname }}'

    pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
        description: '{{ .CommonAnnotations.summary }}'

  - name: 'mlops-alerts'
    email_configs:
      - to: 'ml-team@example.com'
        headers:
          Subject: '[ML Ops] {{ .GroupLabels.alertname }}'

    webhook_configs:
      - url: 'http://mlflow:5000/api/v1/alerts/webhook'

inhibit_rules:
  # Inhibit warning if critical is firing
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'component']

  # Inhibit worker down if entire cluster is down
  - source_match:
      alertname: 'ClusterDown'
    target_match:
      alertname: 'WorkerDown'
